{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmm\n",
    "import time\n",
    "import torch\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from cuml.manifold import UMAP as cuUMAP\n",
    "from cuml.cluster import HDBSCAN as cuHDBSCAN\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:Csci5502@35.222.3.25:5432/reddit-data\") # replace with your own DB\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm.base import Mapped\n",
    "from sqlalchemy.orm import Mapped, declarative_base, mapped_column\n",
    "from sqlalchemy import ARRAY, BigInteger, CHAR, Column, Date, Integer, Numeric, PrimaryKeyConstraint, Sequence, String, Table, Text, UniqueConstraint\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "metadata = Base.metadata\n",
    "\n",
    "\n",
    "class Cluster(Base):\n",
    "    __tablename__ = 'cluster'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='cluster_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger)\n",
    "    cluster_id = mapped_column(Integer)\n",
    "    score_sum = mapped_column(Integer)\n",
    "    day_diff = mapped_column(Integer)\n",
    "    post_date = mapped_column(Date)\n",
    "    num_of_comment = mapped_column(Integer)\n",
    "    subreddit = mapped_column(Text)\n",
    "    list_of_post = mapped_column(ARRAY(Text()))\n",
    "\n",
    "\n",
    "class ClusterInfo(Base):\n",
    "    __tablename__ = 'cluster_info'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='cluster_info_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger)\n",
    "    subreddit = mapped_column(Text)\n",
    "    post_date = mapped_column(Date)\n",
    "    num_of_post = mapped_column(Integer)\n",
    "    avg_num_comment = mapped_column(Numeric)\n",
    "    avg_len_content = mapped_column(Numeric)\n",
    "    avg_score = mapped_column(Numeric)\n",
    "    cluster_id = mapped_column(BigInteger)\n",
    "    number_of_comment = mapped_column(BigInteger)\n",
    "\n",
    "\n",
    "class ClusterInfoNew(Base):\n",
    "    __tablename__ = 'cluster_info_new'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='cluster_info_new_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger, Sequence('cluster_info_id_seq'))\n",
    "    subreddit = mapped_column(Text)\n",
    "    post_date = mapped_column(Date)\n",
    "    num_of_post = mapped_column(Integer)\n",
    "    avg_num_comment = mapped_column(Numeric)\n",
    "    avg_len_content = mapped_column(Numeric)\n",
    "    avg_score = mapped_column(Numeric)\n",
    "    cluster_id = mapped_column(BigInteger)\n",
    "    number_of_comment = mapped_column(BigInteger)\n",
    "\n",
    "\n",
    "class ClusterNew(Base):\n",
    "    __tablename__ = 'cluster_new'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='cluster_new_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger, Sequence('cluster_id_seq'))\n",
    "    cluster_id = mapped_column(Integer)\n",
    "    score_sum = mapped_column(Integer)\n",
    "    day_diff = mapped_column(Integer)\n",
    "    post_date = mapped_column(Date)\n",
    "    num_of_comment = mapped_column(Integer)\n",
    "    subreddit = mapped_column(Text)\n",
    "    list_of_post = mapped_column(ARRAY(Text()))\n",
    "\n",
    "\n",
    "class ClusterTest(Base):\n",
    "    __tablename__ = 'cluster_test'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='cluster_test_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger, Sequence('cluster_id_seq'))\n",
    "    cluster_id = mapped_column(Integer)\n",
    "    score_sum = mapped_column(Integer)\n",
    "    day_diff = mapped_column(Integer)\n",
    "    post_date = mapped_column(Date)\n",
    "    num_of_comment = mapped_column(Integer)\n",
    "    subreddit = mapped_column(Text)\n",
    "    list_of_post = mapped_column(ARRAY(Text()))\n",
    "\n",
    "\n",
    "t_economics = Table(\n",
    "    'economics', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='economics_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "t_economics_new = Table(\n",
    "    'economics_new', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='economics_new_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "t_politics = Table(\n",
    "    'politics', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='politics_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "t_politics_2 = Table(\n",
    "    'politics_2', metadata,\n",
    "    Column('private_id', Integer, Sequence('politics_private_id_seq'), nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='politics_2_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "t_politics_new = Table(\n",
    "    'politics_new', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='politics_new_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "class PostPreprocessing(Base):\n",
    "    __tablename__ = 'post_preprocessing'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='post_preprocessing_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger)\n",
    "    post_id = mapped_column(String)\n",
    "    subreddit = mapped_column(String)\n",
    "    post_create = mapped_column(Date)\n",
    "    content = mapped_column(String)\n",
    "    content_length = mapped_column(BigInteger)\n",
    "    num_of_comment = mapped_column(BigInteger)\n",
    "\n",
    "\n",
    "class Sentiment(Base):\n",
    "    __tablename__ = 'sentiment'\n",
    "    __table_args__ = (\n",
    "        PrimaryKeyConstraint('id', name='sentiment_pkey'),\n",
    "    )\n",
    "\n",
    "    id = mapped_column(BigInteger)\n",
    "    post_id = mapped_column(CHAR(64))\n",
    "    result = mapped_column(String(65535))\n",
    "\n",
    "\n",
    "t_sports = Table(\n",
    "    'sports', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='sports_private_id_key')\n",
    ")\n",
    "\n",
    "\n",
    "t_sports_new = Table(\n",
    "    'sports_new', metadata,\n",
    "    Column('private_id', Integer, nullable=False),\n",
    "    Column('post_id', String(8), nullable=False),\n",
    "    Column('subreddit', String(25)),\n",
    "    Column('post_title', Text),\n",
    "    Column('post_content', Text),\n",
    "    Column('post_score', Integer),\n",
    "    Column('post_create', Date),\n",
    "    Column('command_content', ARRAY(Text())),\n",
    "    Column('command_score', ARRAY(Integer())),\n",
    "    Column('command_create', ARRAY(Date())),\n",
    "    UniqueConstraint('private_id', name='sports_new_private_id_key')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize(v):\n",
    "    \"\"\"Safely normalize a vector to unit length.\"\"\"\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v / norm if norm > 0 else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Ensure that NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zhihaolyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "class GPUEmbeddingProcessor:\n",
    "    def __init__(self, \n",
    "                 model_name='allenai/longformer-base-4096', \n",
    "                 batch_size=32):\n",
    "        # Use RMM pool allocator (updated method)\n",
    "        rmm.reinitialize(\n",
    "            pool_allocator=True,  # Enable pool allocation\n",
    "            initial_pool_size=2**30  # 1 GB initial pool size, adjust as needed\n",
    "        )\n",
    "        \n",
    "        # Detect device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # Batch processing configuration\n",
    "        self.batch_size = batch_size\n",
    "        self.stop_words = set(stopwords.words('english'))  # Add language of your choice\n",
    "\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokenize text and filter out stopwords\n",
    "        tokens = text.split()  # Assuming the text is pre-tokenized or you can use nltk.word_tokenize\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_tokens)  # Return as a cleaned-up string\n",
    "    \n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        # Preprocess texts into batches\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Processing\", unit=\"batch\"):\n",
    "            batch_texts = texts[i:i+self.batch_size]\n",
    "\n",
    "            cleaned_batch_texts = [self.remove_stopwords(text) for text in batch_texts]\n",
    "            try:\n",
    "                # Tokenize batch\n",
    "                inputs = self.tokenizer(\n",
    "                    cleaned_batch_texts, \n",
    "                    return_tensors='pt', \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=1024\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                \n",
    "                # Mean pooling\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "                \n",
    "                # Convert to numpy and normalize\n",
    "                batch_embeddings_np = batch_embeddings.cpu().numpy()\n",
    "                batch_normalized = np.array([safe_normalize(emb) for emb in batch_embeddings_np])\n",
    "                \n",
    "                all_embeddings.append(batch_normalized)\n",
    "                \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "\n",
    "        \n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_dimensionality_reduction_clustering(embeddings, \n",
    "                                            n=2,\n",
    "                                            n_neighbors=15, \n",
    "                                            min_cluster_size=10):\n",
    "    try:\n",
    "        # Convert to CuPy array\n",
    "        embeddings_gpu = cp.asarray(embeddings)\n",
    "        # GPU UMAP\n",
    "        umap_reducer = cuUMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n,\n",
    "            random_state=42\n",
    "        )\n",
    "        reduced_dims = umap_reducer.fit_transform(embeddings_gpu)\n",
    "        # GPU HDBSCAN\n",
    "        hdbscan_clusterer = cuHDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            gen_min_span_tree=True,\n",
    "            min_samples = 20\n",
    "        )\n",
    "        cluster_labels = hdbscan_clusterer.fit_predict(reduced_dims)\n",
    "\n",
    "        t1 = cp.asnumpy(reduced_dims)\n",
    "        t2 = cp.asnumpy(cluster_labels)\n",
    "        return (\n",
    "            t1, \n",
    "            t2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Dimensionality reduction error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_reddit = set(['politics', 'PoliticalDiscussion', 'unpopularopinion', 'Conservative', 'PoliticalHumor'])\n",
    "s_reddit = set(['nba','sports','nfl','PremierLeague','formula1'])\n",
    "e_reddit = set(['Economics', 'AskEconomics', 'inflation', 'economicCollapse', 'badeconomics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_db(df, engine):\n",
    "    df_new = df.drop(columns=['command_score', 'post_create', 'post_score', 'command_score_sum', 'command_content'])\n",
    "    df_new.to_sql('cluster', engine, index=False, if_exists='append') # cluster is the table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(df, month, year=2023):\n",
    "    # Add cluster information to DataFrame\n",
    "    \n",
    "    agg_func = {\n",
    "        'post_id': list,\n",
    "        'command_score': list,\n",
    "        'post_create': list,\n",
    "        'post_score': 'sum',\n",
    "        'subreddit': 'first',\n",
    "        \"command_content\": list\n",
    "    }\n",
    "    result_df = df.groupby('cluster_id').agg(agg_func).reset_index()\n",
    "\n",
    "    result_df = result_df.rename(columns={'post_id': 'list_of_post'}) \n",
    "\n",
    "    result_df['command_score_sum'] = result_df['command_score'].apply(lambda a : sum([0 if item is None else item for sublist in a for item in sublist]))\n",
    "    result_df['score_sum'] = result_df['command_score_sum'] + result_df['post_score']\n",
    "    \n",
    "    result_df['num_of_comment'] = result_df['command_content'].apply(lambda a : sum([0 if item is None else 1 for sublist in a for item in sublist]))\n",
    "    \n",
    "    result_df['post_create'] = result_df['post_create'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))\n",
    "    result_df['day_diff'] = result_df['post_create'].apply(lambda x: x.max() - x.min())\n",
    "    result_df['day_diff'] = result_df['day_diff'].dt.days\n",
    "    \n",
    "    def classify_subreddit(sub):\n",
    "        if sub in p_reddit:\n",
    "            return 'politic'\n",
    "        elif sub in s_reddit:\n",
    "            return 'sport'\n",
    "        else:\n",
    "            return 'economic'\n",
    "    result_df['subreddit'] = result_df['subreddit'].apply(classify_subreddit)\n",
    "    \n",
    "    result_df['post_date'] = pd.to_datetime(f'{year}-{month:02d}')\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 166/166 [06:35<00:00,  2.38s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ============ 4 + 5 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 5}\n",
      "Best Silhouette Score: 0.5123909910519918\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 4 + 10 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 15}\n",
      "Best Silhouette Score: 0.48202627897262573\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 4 + 15 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 5}\n",
      "Best Silhouette Score: 0.4705037971337636\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 4 + 20 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 40, 'min_samples': 15}\n",
      "Best Silhouette Score: 0.45420345664024353\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 8 + 5 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 30, 'min_samples': 15}\n",
      "Best Silhouette Score: 0.537553052107493\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 8 + 10 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 20, 'min_samples': 25}\n",
      "Best Silhouette Score: 0.48207007845242816\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 8 + 15 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 15}\n",
      "Best Silhouette Score: 0.4680761496225993\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 8 + 20 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 40, 'min_samples': 25}\n",
      "Best Silhouette Score: 0.45178961753845215\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 16 + 5 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 10, 'min_samples': 25}\n",
      "Best Silhouette Score: 0.5141873061656952\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 16 + 10 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 30, 'min_samples': 25}\n",
      "Best Silhouette Score: 0.4897751311461131\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 16 + 15 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 5}\n",
      "Best Silhouette Score: 0.4706255992253621\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 16 + 20 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 20, 'min_samples': 35}\n",
      "Best Silhouette Score: 0.46156612038612366\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 24 + 5 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 10, 'min_samples': 25}\n",
      "Best Silhouette Score: 0.5236858328183492\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 24 + 10 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 30, 'min_samples': 5}\n",
      "Best Silhouette Score: 0.4872885048389435\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 24 + 15 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 50, 'min_samples': 10}\n",
      "Best Silhouette Score: 0.46979549527168274\n",
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "| ============ 24 + 20 ============= |\n",
      "Best Parameters: {'alpha': 1.0, 'cluster_selection_method': 'eom', 'min_cluster_size': 5, 'min_samples': 35}\n",
      "Best Silhouette Score: 0.46234463651974994\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "auto_reply = \"Please remember what subreddit you are in, this is unpopular opinion. We want civil and unpopular takes and discussion. Any uncivil and ToS violating comments will be removed and subject to a ban. Have a nice day!*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to= if you have any questions or concerns.*\"\n",
    "\n",
    "reduction_dimentions = 12\n",
    "n = 10\n",
    "\n",
    "year = 2023\n",
    "for month in range(1, 13):\n",
    "    # GPU-accelerated CSV reading with cuDF\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM public.sports\n",
    "    WHERE EXTRACT(YEAR FROM post_create) = {year}\n",
    "    AND EXTRACT(MONTH FROM post_create) = {month}\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    # 定义一个函数来处理每一行\n",
    "    def combine_content(row):\n",
    "        # 如果 command_content 是 [None]，只返回 post_content\n",
    "        if row['command_content'] == [None] or row['command_content'] is None:\n",
    "            return row['post_content']\n",
    "        # 否则将 post_content 和 command_content 合并\n",
    "        else:\n",
    "            original_text = row['post_content']\n",
    "            for i in range(len(row['command_content'])):\n",
    "                text = row['command_content'][i]\n",
    "                if text is not None and text != auto_reply:\n",
    "                    original_text += \" \"\n",
    "                    original_text += text\n",
    "            return original_text\n",
    "        \n",
    "    df['combined_content'] = df.apply(combine_content, axis=1)\n",
    "    # Convert to list for processing\n",
    "    texts = df['combined_content'].tolist()\n",
    "\n",
    "    # Initialize GPU embedding processor\n",
    "    embedding_processor = GPUEmbeddingProcessor(batch_size=32)\n",
    "\n",
    "    # # Generate embeddings\n",
    "    embeddings = embedding_processor.generate_embeddings(texts)\n",
    "\n",
    "    # # Perform GPU dimensionality reduction and clustering\n",
    "    reduced_dims, cluster_labels = gpu_dimensionality_reduction_clustering(\n",
    "        embeddings, \n",
    "        n=reduction_dimentions,\n",
    "        n_neighbors=n,\n",
    "        min_cluster_size=5\n",
    "    )\n",
    "\n",
    "    df['cluster_id'] = cluster_labels\n",
    "    result_df = aggregation(df, month)\n",
    "\n",
    "    save_to_db(result_df, engine)\n",
    "\n",
    "    # Print cluster information\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "    print(f\"Number of clusters detected: {len(unique_clusters)}\")\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_count = np.sum(cluster_labels == cluster)\n",
    "        print(f\"Cluster {cluster}: {cluster_count} texts\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_cost = end_time - start_time\n",
    "    print(f\"this task costs {time_cost} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
